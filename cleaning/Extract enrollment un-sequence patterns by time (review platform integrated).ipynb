{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. url-subject map 생성.\n",
    "### 1. 먼저 class-central review platform 돌면서 pattern 추출,\n",
    "### 2. 다음으로 coursetalk review platform.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sort_by_num(x):\n",
    "    if x==\"id\":\n",
    "        return 0\n",
    "    else:\n",
    "        return int(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subjectMap = {}\n",
    "with open(\"./MOOC_pattern_extraction/url_subject_info.csv\", \"r\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        subjectMap[row['url']] = row['subject']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3153\n"
     ]
    }
   ],
   "source": [
    "## read course Data set\n",
    "courseList = []\n",
    "with open(\"./MOOC_data/xtData/(cleand)ClassCentral_course.csv\", \"r\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        courseList.append(row)\n",
    "        \n",
    "print(len(courseList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20903\n"
     ]
    }
   ],
   "source": [
    "## read review Data set\n",
    "reviewList = []\n",
    "with open(\"./MOOC_data/xtData/(Date_Adjusted)ClassCentral_review_renewed.csv\", \"r\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        reviewList.append(row)\n",
    "print(len(reviewList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Extract Pattern </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 사용자, 사용자의 수강리스트 만들기.\n",
    "\n",
    "reviewers =[]\n",
    "for review in reviewList:\n",
    "    \n",
    "    \n",
    "    # reviewer id가 없을 경우 continue\n",
    "    if review['reviewer_url'] == \"null\":\n",
    "        continue\n",
    "        \n",
    "    \n",
    "    # reviewers list 를 돌면서 course 추가\n",
    "    flag = False\n",
    "    for index, re in enumerate(reviewers):\n",
    "        \n",
    "        \n",
    "        if re['reviewer'] == review['reviewer_url']:\n",
    "            reviewers[index]['courses'].append(dict({\n",
    "                    'date': review['review_date'],\n",
    "                    'course': review['url'],\n",
    "                    \n",
    "                \n",
    "                }))\n",
    "            flag = True\n",
    "            \n",
    "            \n",
    "    \n",
    "    if flag == False:\n",
    "        reviewers.append(dict({\n",
    "            'reviewer': review['reviewer_url'],\n",
    "            'courses': [dict({\n",
    "                'date': review['review_date'],\n",
    "                'course': review['url'],\n",
    "                \n",
    "            })]\n",
    "            \n",
    "        }))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_t1_trans = []\n",
    "final_t2_trans = []\n",
    "final_t3_trans = []\n",
    "final_t4_trans = []\n",
    "\n",
    "# course directed edge list 생성\n",
    "for re in reviewers:\n",
    "\n",
    "    t1_nodes = []\n",
    "    t2_nodes = []\n",
    "    t3_nodes = []\n",
    "    t4_nodes = []\n",
    "       \n",
    "    for co in re['courses']:\n",
    "\n",
    "               \n",
    "        date = datetime.datetime.strptime(co['date'], \"%Y-%m-%d\") \n",
    "        \n",
    "        if date.year == 2013:\n",
    "\n",
    "            # T1: \n",
    "            if date.month > 6 and date.month <= 12:\n",
    "                t1_nodes.append(dict({\n",
    "                    'course': co['course'],\n",
    "                    'date': co['date']           \n",
    "                }))\n",
    "        \n",
    "        elif date.year == 2014:\n",
    "            \n",
    "            if date.month > 0 and date.month < 7:\n",
    "                t1_nodes.append(dict({\n",
    "                    'course': co['course'],\n",
    "                    'date': co['date']           \n",
    "                }))\n",
    "                \n",
    "            # T2: \n",
    "            else:\n",
    "                t2_nodes.append(dict({\n",
    "                    'course': co['course'],\n",
    "                    'date': co['date']           \n",
    "                }))\n",
    "                \n",
    "\n",
    "        elif date.year == 2015:\n",
    "            \n",
    "            if date.month > 0 and date.month < 7:\n",
    "                t2_nodes.append(dict({\n",
    "                    'course': co['course'],\n",
    "                    'date': co['date']           \n",
    "                }))\n",
    "                \n",
    "            # T3: 15.7~12\n",
    "            else:\n",
    "                t3_nodes.append(dict({\n",
    "                    'course': co['course'],\n",
    "                    'date': co['date']           \n",
    "                }))\n",
    "                \n",
    "        elif date.year == 2016:\n",
    "            \n",
    "            if date.month > 0 and date.month < 7:\n",
    "                t3_nodes.append(dict({\n",
    "                    'course': co['course'],\n",
    "                    'date': co['date']           \n",
    "                }))\n",
    "                \n",
    "            # T4\n",
    "            else:\n",
    "                t4_nodes.append(dict({\n",
    "                    'course': co['course'],\n",
    "                    'date': co['date']           \n",
    "                }))\n",
    "                \n",
    "        elif date.year == 2017:\n",
    "            \n",
    "            if date.month > 0 and date.month < 7:\n",
    "            \n",
    "                t4_nodes.append(dict({\n",
    "                        'course': co['course'],\n",
    "                        'date': co['date']           \n",
    "                    }))\n",
    "    \n",
    "    # 날짜별 sorting\n",
    "    t1_nodes.sort(key=lambda item:item['date'], reverse=False)\n",
    "    t2_nodes.sort(key=lambda item:item['date'], reverse=False)\n",
    "    t3_nodes.sort(key=lambda item:item['date'], reverse=False)\n",
    "    t4_nodes.sort(key=lambda item:item['date'], reverse=False)\n",
    "    \n",
    "    # 같은 vertex time에 있는 노드가 두개 이상인지 체크, 2개 이상일 경우 date 비교 후 edgelist에 추가\n",
    "    temp_dict1 = extract_pattern(t1_nodes, re['reviewer'])\n",
    "    temp_dict2 = extract_pattern(t2_nodes, re['reviewer'])\n",
    "    temp_dict3 = extract_pattern(t3_nodes, re['reviewer'])\n",
    "    temp_dict4 = extract_pattern(t4_nodes, re['reviewer'])\n",
    "    if temp_dict1:\n",
    "        final_t1_trans.append(temp_dict1)\n",
    "    if temp_dict2:\n",
    "        final_t2_trans.append(temp_dict2)\n",
    "    if temp_dict3:\n",
    "        final_t3_trans.append(temp_dict3)\n",
    "    if temp_dict4:\n",
    "        final_t4_trans.append(temp_dict4)\n",
    "    \n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transcript_to_subject_mapping(transList):\n",
    "    \n",
    "    for index, trans in enumerate(transList):\n",
    "        \n",
    "        for j, item in enumerate(trans['item-set']):\n",
    "            \n",
    "            if item not in subjectMap.keys():\n",
    "                transList[index]['item-set'].pop(j)\n",
    "                if len(transList[index]['item-set']) == 0: # pop 이후의 transcript list 길이가 0인 경우 해당 transcript 삭제\n",
    "                    transList.pop(index)\n",
    "            else:\n",
    "                transList[index]['item-set'][j] = subjectMap[item]\n",
    "                               \n",
    "       \n",
    "    return transList;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_pattern(nodes, reviewer_id):\n",
    "    \n",
    "    tempDict = {}\n",
    "    # nodes 사이즈가 0인 경우 return\n",
    "    if len(nodes) == 0:\n",
    "        return tempDict\n",
    "#     print(reviewer_id)\n",
    "    # 사이즈가 1이상인 경우\n",
    "    tempDict = {\n",
    "            'id': reviewer_id,\n",
    "            'item-set': []\n",
    "    }\n",
    "    for i, node in enumerate(nodes):\n",
    "#         print(node)\n",
    "        tempDict['item-set'].append(node['course'])\n",
    "        \n",
    "    return tempDict    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t1_trans_final = transcript_to_subject_mapping(final_t1_trans)\n",
    "t2_trans_final = transcript_to_subject_mapping(final_t2_trans)\n",
    "t3_trans_final = transcript_to_subject_mapping(final_t3_trans)\n",
    "t4_trans_final = transcript_to_subject_mapping(final_t4_trans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write sequence list\n",
    "fieldnames = ['id', 'item-set']\n",
    "with open('./MOOC_pattern_extraction/classCentral_transaction_t1.csv', 'w', newline='') as write_file:\n",
    "    dict_writer = csv.DictWriter(write_file, fieldnames)\n",
    "    dict_writer.writeheader()\n",
    "    for row in t1_trans_final:\n",
    "        dict_writer.writerow(row)\n",
    "        \n",
    "# write sequence list\n",
    "with open('./MOOC_pattern_extraction/classCentral_transaction_t2.csv', 'w', newline='') as write_file:\n",
    "\n",
    "    dict_writer = csv.DictWriter(write_file, fieldnames)\n",
    "    dict_writer.writeheader()\n",
    "    for row in t2_trans_final:\n",
    "        dict_writer.writerow(row)\n",
    "        \n",
    "# write sequence list\n",
    "with open('./MOOC_pattern_extraction/classCentral_transaction_t3.csv', 'w', newline='') as write_file:\n",
    "\n",
    "    dict_writer = csv.DictWriter(write_file, fieldnames)\n",
    "    dict_writer.writeheader()\n",
    "    for row in t3_trans_final:\n",
    "        dict_writer.writerow(row)\n",
    "        \n",
    "# write sequence list\n",
    "with open('./MOOC_pattern_extraction/classCentral_transaction_t4.csv', 'w', newline='') as write_file:\n",
    "\n",
    "    dict_writer = csv.DictWriter(write_file, fieldnames)\n",
    "    dict_writer.writeheader()\n",
    "    for row in t4_trans_final:\n",
    "        dict_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract pattern at courseTalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3331\n",
      "23604\n"
     ]
    }
   ],
   "source": [
    "# data load\n",
    "\n",
    "courseList = []\n",
    "with open(\"./MOOC_data/xtData/(cleand_added_missing)Coursetalk_course.csv\", \"r\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        row['url'] = row['url'].replace(\"/enroll?course_id=\", \"\")\n",
    "        row['url'] = row['url'].replace(\"&direct_enroll=true\", \"\")\n",
    "           \n",
    "        courseList.append(row)\n",
    "        \n",
    "print(len(courseList))\n",
    "\n",
    "## read review Data set\n",
    "reviewList = []\n",
    "with open(\"./MOOC_data/xtData/CourseTalk_review_renewed.csv\", \"r\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        row['review_value'] = int(row['review_value'])/2 # 0~10 scale이므로 2로 나누어 0~5 scale로 변환.\n",
    "        reviewList.append(row)\n",
    "print(len(reviewList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 사용자, 사용자의 수강리스트 만들기.\n",
    "\n",
    "reviewers =[]\n",
    "for review in reviewList:\n",
    "    \n",
    "    \n",
    "    # reviewer id가 없을 경우 continue\n",
    "    if review['reviewer_url'] == \"null\":\n",
    "        continue\n",
    "        \n",
    "    \n",
    "    # reviewers list 를 돌면서 course 추가\n",
    "    flag = False\n",
    "    for index, re in enumerate(reviewers):\n",
    "                \n",
    "        if re['reviewer'] == review['reviewer_url']:\n",
    "            reviewers[index]['courses'].append(dict({\n",
    "                    'date': review['review_date'],\n",
    "                    'course': review['course_id']\n",
    "                }))\n",
    "            flag = True\n",
    "            \n",
    "            \n",
    "    \n",
    "    if flag == False:\n",
    "        reviewers.append(dict({\n",
    "            'reviewer': review['reviewer_url'],\n",
    "            'courses': [dict({\n",
    "                'date': review['review_date'],\n",
    "                'course': review['course_id']\n",
    "            })]\n",
    "            \n",
    "        }))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_t1_trans = []\n",
    "final_t2_trans = []\n",
    "final_t3_trans = []\n",
    "final_t4_trans = []\n",
    "\n",
    "# course directed edge list 생성\n",
    "for re in reviewers:\n",
    "\n",
    "    t1_nodes = []\n",
    "    t2_nodes = []\n",
    "    t3_nodes = []\n",
    "    t4_nodes = []\n",
    "       \n",
    "    for co in re['courses']:\n",
    "\n",
    "               \n",
    "        date = datetime.datetime.strptime(co['date'], \"%Y-%m-%d\") \n",
    "        \n",
    "        if date.year == 2013:\n",
    "\n",
    "            # T1: \n",
    "            if date.month > 6 and date.month <= 12:\n",
    "                t1_nodes.append(dict({\n",
    "                    'course': co['course'],\n",
    "                    'date': co['date']           \n",
    "                }))\n",
    "        \n",
    "        elif date.year == 2014:\n",
    "            \n",
    "            if date.month > 0 and date.month < 7:\n",
    "                t1_nodes.append(dict({\n",
    "                    'course': co['course'],\n",
    "                    'date': co['date']           \n",
    "                }))\n",
    "                \n",
    "            # T2: \n",
    "            else:\n",
    "                t2_nodes.append(dict({\n",
    "                    'course': co['course'],\n",
    "                    'date': co['date']           \n",
    "                }))\n",
    "                \n",
    "\n",
    "        elif date.year == 2015:\n",
    "            \n",
    "            if date.month > 0 and date.month < 7:\n",
    "                t2_nodes.append(dict({\n",
    "                    'course': co['course'],\n",
    "                    'date': co['date']           \n",
    "                }))\n",
    "                \n",
    "            # T3: 15.7~12\n",
    "            else:\n",
    "                t3_nodes.append(dict({\n",
    "                    'course': co['course'],\n",
    "                    'date': co['date']           \n",
    "                }))\n",
    "                \n",
    "        elif date.year == 2016:\n",
    "            \n",
    "            if date.month > 0 and date.month < 7:\n",
    "                t3_nodes.append(dict({\n",
    "                    'course': co['course'],\n",
    "                    'date': co['date']           \n",
    "                }))\n",
    "                \n",
    "            # T4\n",
    "            else:\n",
    "                t4_nodes.append(dict({\n",
    "                    'course': co['course'],\n",
    "                    'date': co['date']           \n",
    "                }))\n",
    "                \n",
    "        elif date.year == 2017:\n",
    "            \n",
    "            if date.month > 0 and date.month < 7:\n",
    "            \n",
    "                t4_nodes.append(dict({\n",
    "                        'course': co['course'],\n",
    "                        'date': co['date']           \n",
    "                    }))\n",
    "    \n",
    "    # 날짜별 sorting\n",
    "    t1_nodes.sort(key=lambda item:item['date'], reverse=False)\n",
    "    t2_nodes.sort(key=lambda item:item['date'], reverse=False)\n",
    "    t3_nodes.sort(key=lambda item:item['date'], reverse=False)\n",
    "    t4_nodes.sort(key=lambda item:item['date'], reverse=False)\n",
    "    \n",
    "    # 같은 vertex time에 있는 노드가 두개 이상인지 체크, 2개 이상일 경우 date 비교 후 edgelist에 추가\n",
    "    temp_dict1 = extract_pattern(t1_nodes, re['reviewer'])\n",
    "    temp_dict2 = extract_pattern(t2_nodes, re['reviewer'])\n",
    "    temp_dict3 = extract_pattern(t3_nodes, re['reviewer'])\n",
    "    temp_dict4 = extract_pattern(t4_nodes, re['reviewer'])\n",
    "    if temp_dict1:\n",
    "        final_t1_trans.append(temp_dict1)\n",
    "    if temp_dict2:\n",
    "        final_t2_trans.append(temp_dict2)\n",
    "    if temp_dict3:\n",
    "        final_t3_trans.append(temp_dict3)\n",
    "    if temp_dict4:\n",
    "        final_t4_trans.append(temp_dict4)\n",
    "    \n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t1_trans_final_courseTalk = transcript_to_subject_mapping(final_t1_trans)\n",
    "t2_trans_final_courseTalk = transcript_to_subject_mapping(final_t2_trans)\n",
    "t3_trans_final_courseTalk = transcript_to_subject_mapping(final_t3_trans)\n",
    "t4_trans_final_courseTalk = transcript_to_subject_mapping(final_t4_trans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write sequence list\n",
    "fieldnames = ['id', 'item-set']\n",
    "        \n",
    "with open('./MOOC_pattern_extraction/courseTalk_transaction_t1.csv', 'w', newline='') as write_file:\n",
    "    dict_writer = csv.DictWriter(write_file, fieldnames)\n",
    "    dict_writer.writeheader()\n",
    "    for row in t1_trans_final_courseTalk:\n",
    "        dict_writer.writerow(row)\n",
    "        \n",
    "# write sequence list\n",
    "with open('./MOOC_pattern_extraction/courseTalk_transaction_t2.csv', 'w', newline='') as write_file:\n",
    "    dict_writer = csv.DictWriter(write_file, fieldnames)\n",
    "    dict_writer.writeheader()\n",
    "    for row in t2_trans_final_courseTalk:\n",
    "        dict_writer.writerow(row)\n",
    "        \n",
    "# write sequence list\n",
    "with open('./MOOC_pattern_extraction/courseTalk_transaction_t3.csv', 'w', newline='') as write_file:\n",
    "    dict_writer = csv.DictWriter(write_file, fieldnames)\n",
    "    dict_writer.writeheader()\n",
    "    for row in t3_trans_final_courseTalk:\n",
    "        dict_writer.writerow(row)\n",
    "        \n",
    "# write sequence list\n",
    "with open('./MOOC_pattern_extraction/courseTalk_transaction_t4.csv', 'w', newline='') as write_file:\n",
    "    dict_writer = csv.DictWriter(write_file, fieldnames)\n",
    "    dict_writer.writeheader()\n",
    "    for row in t4_trans_final_courseTalk:\n",
    "        dict_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrate transaction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t1_trans_final.extend(t1_trans_final_courseTalk)\n",
    "t2_trans_final.extend(t2_trans_final_courseTalk)\n",
    "t3_trans_final.extend(t3_trans_final_courseTalk)\n",
    "t4_trans_final.extend(t4_trans_final_courseTalk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write sequence list\n",
    "fieldnames = ['id', 'item-set']\n",
    "with open('./MOOC_pattern_extraction/integrated_transaction_t1.csv', 'w', newline='') as write_file:\n",
    "    dict_writer = csv.DictWriter(write_file, fieldnames)\n",
    "    dict_writer.writeheader()\n",
    "    for row in t1_trans_final:\n",
    "        dict_writer.writerow(row)\n",
    "        \n",
    "# write sequence list\n",
    "with open('./MOOC_pattern_extraction/courseTalk_transaction_t2.csv', 'w', newline='') as write_file:\n",
    "    \n",
    "    dict_writer = csv.DictWriter(write_file, fieldnames)\n",
    "    dict_writer.writeheader()\n",
    "    for row in t2_trans_final:\n",
    "        dict_writer.writerow(row)\n",
    "        \n",
    "# write sequence list\n",
    "with open('./MOOC_pattern_extraction/courseTalk_transaction_t3.csv', 'w', newline='') as write_file:\n",
    "    dict_writer = csv.DictWriter(write_file, fieldnames)\n",
    "    dict_writer.writeheader()\n",
    "    for row in t3_trans_final:\n",
    "        dict_writer.writerow(row)\n",
    "        \n",
    "# write sequence list\n",
    "with open('./MOOC_pattern_extraction/courseTalk_transaction_t4.csv', 'w', newline='') as write_file:\n",
    "    dict_writer = csv.DictWriter(write_file, fieldnames)\n",
    "    dict_writer.writeheader()\n",
    "    for row in t4_trans_final:\n",
    "        dict_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract frequency of each transaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 먼저 각 item-set string 정렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def itemset_sort(trans_list):\n",
    "    for index, tr in enumerate(trans_list):\n",
    "        trans_list[index]['item-set'].sort()\n",
    "        \n",
    "    return trans_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_trans_sorted = itemset_sort(t1_trans_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '/@dhawal',\n",
       " 'item-set': ['Algorithms',\n",
       "  'Business Strategy',\n",
       "  'Data Analysis',\n",
       "  'Economics',\n",
       "  'Software Development']}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1_trans_sorted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['he','she'] == ['he', 'she']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
